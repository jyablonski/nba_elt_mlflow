# dvc

Pretty basic stuff. dvc gives you the capability to store data files in S3 so you don't have to check them into git, and in their place you just store metadata files (`*.dvc`)about the data files themselves in git. Then you can pull them from S3 down to local file directory with a few dvc commands, as long as you have permissions.

dvc also gives you the capability to run reproducible ML Training + Experiments. It also tracks the outputs and caches the results of things.

Metadata about your S3 Bucket goes in `.dvc/config`. I put data files into `data/`, and I tracked experiment results into that `data/` folder as well into uniquely named experiments such as `data/first_test/`.

To create reproducible ML Experiments, you can use the `dvc stage` command and pass in the name of your stage, the Python File it runs, any data files it uses for training, and the file path for the output files.

The `dvc.yaml` File contains info about your pipelines as you add stages to it via `dvc stage`. This file is generated by dvc and doesn't need to be modified by users.

You can run the pipeline with `dvc repro`. When you do this, a `dvc.lock` file will be auto-generated w/ metadata about the run. This file is generated by dvc and doesn't need to be modified by users.

Afterwards you can run `dvc push` to push results up to remote storage. If you ever lose the files locally, you can run `dvc pull` and it should be able to pull them back down.

![image](https://github.com/user-attachments/assets/9924db16-16ec-4602-8866-8bd0bdb4c333)

- In S3, you can't really follow along with the file names as it uses metadata file names found in `.dvc/cache/files/md5/**` on the User's machine who has the files or added the stages and ran them locally.
- So basically, you shouldn't need to go digging around in S3 ever I don't think.

## dvc Code Walkthrough
``` sh
# initialize project 
dvc init

# add s3 bucket to config - give it a name `myremote`
dvc remote add -d myremote s3://jyablonski-dvc-praq

# modify params for the remote
dvc remote modify myremote region 'us-east-2'

dvc config core.autostage true

# csvs are gitignored w/ `data/*.csv`, but the .dvc files arent.
# so you add your files to this path locally, then run dvc add, then dvc push to push them up to s3.
dvc add data/past_games_2023-10-18.csv

dvc push

# dvc remove doesn't remove files from the DVC cache or remote storage. Use dvc gc for that.
dvc remove data/past_games_2023-10-18.csv.dvc

# this deletes unused files locally, but not from remote storage.
dvc gc --workspace

# deletes unused files from remote storage
# there's a lot going on here though because you might need this file for past experiments or something
# and storage is cheap. so generally, i'd argue you dont need to run this all that often.
dvc gc --workspace --cloud

# this creates a file `dvc.yaml` - this is autogenerated by the dvc stage commands
# you can add it optional parameters here too
dvc stage add -n first_test \
                -d practice/dvc_training.py -d data/past_games_2023-10-18.csv \
                -o data/first_test \
                python practice/dvc_training.py data/past_games_2023-10-18.csv

# see the flow of your pipeline in terminal output
dvc dag

# actually run the pipeline
dvc repro

# push results from the pipeline up to remote storage
dvc push

# get files back from remote storage into local storage
dvc pull
```

## dvc Parameters
[Link](https://dvc.org/doc/command-reference/stage/add#example-using-parameter-dependencies)

You can use the dvc package to basically read in parameters from a `params.yaml` file into your Python Script that's doing the ML Training.

``` py
import dvc.api

params = dvc.api.params_show()

seed = params['seed']
lr = params['train']['lr']
epochs = params['train']['epochs']
```